{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee34b7f-5cff-472b-a0a7-7d7abe679279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09e1f4de-a37a-4824-a4e5-c828ab1a1671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c19eb09c-b473-401a-a499-863e90ae7811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING MEDIAPIPE TO GENERATE LANDMARKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10683c8a-f2da-443d-976c-2aee5b311fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting mediapipe holistics\n",
    "#we are creating 2 variables ,one for mediapipe holistic and second one for mediapipe drawing utilities\n",
    "#mediapipe holistoc and mediapipe drawing utilites are 2 components in the mediapipe \n",
    "#mediapipe holistic helps in pose estimation  whereas the mediapipe drawing utilities helps in visualizing the results for the developers\n",
    "holistics =mp.solutions.holistic #imports the holistic module from the solutions which is a sub module in mediapipe\n",
    "drawings=mp.solutions.drawing_utils #imports the drawing module from the solutions which is a sub module in mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39348752-287d-4d94-8e0d-0ab3054d472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp_detect(im,model):   #setting a function for detection.here model is mediapipe holistic model\n",
    "    im=cv2.cvtColor(im,cv2.COLOR_BGR2RGB)  #converts im from BGR 2 RGB color space.cvtcolor() is function that changes imag\n",
    "    im.flags.writeable=False   #sets im unwriteable so that the mediapipe won't the im data during processing\n",
    "    results=model.process(im)  #process() is a function . It processes the im using the holistic model\n",
    "    im.flags.writeable=True    #sets back im to writeable after processing\n",
    "    im=cv2.cvtColor(im,cv2.COLOR_RGB2BGR)  #converts im from RGB to BGR color space\n",
    "    return im,results          #return the processed im,detection results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e88fd710-e17c-455b-a6e4-4c9f527a8d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def landmarks_style(im,results):#styling the landmarks like size,color etc\n",
    "    # Draw face connections and styling face connections\n",
    "    drawings.draw_landmarks(im, results.face_landmarks, holistics.FACEMESH_TESSELATION,\n",
    "                             drawings.DrawingSpec(color=(80,110,10),thickness=1,circle_radius=1),\n",
    "                             drawings.DrawingSpec(color=(80,256,121),thickness=1,circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections  and styling pose connections\n",
    "    drawings.draw_landmarks(im, results.pose_landmarks, holistics.POSE_CONNECTIONS,\n",
    "                              drawings.DrawingSpec(color=(80,22,10),thickness=2,circle_radius=4),\n",
    "                               drawings.DrawingSpec(color=(80,44,121),thickness=2,circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections  and styling left hand connections\n",
    "    drawings.draw_landmarks(im, results.left_hand_landmarks, holistics.HAND_CONNECTIONS,\n",
    "                              drawings.DrawingSpec(color=(121,22,76),thickness=2,circle_radius=4),\n",
    "                               drawings.DrawingSpec(color=(121,44,250),thickness=2,circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  and styling right hand connections\n",
    "    drawings.draw_landmarks(im, results.right_hand_landmarks, holistics.HAND_CONNECTIONS,\n",
    "                              drawings.DrawingSpec(color=(245,117,66),thickness=2,circle_radius=4),\n",
    "                              drawings.DrawingSpec(color=(245,66,230),thickness=2,circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08d042b-b4e6-4e10-9caa-e32833849fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap =cv2.VideoCapture(0)  #accesing webcam.default index of webcam is 0,storing everything got from video in cap avriable\n",
    "with holistics.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():                   #checking whether the webcam is open or not.so used isOpened() function \n",
    "        ret,frame=cap.read()                #reading the feed.while reading, we are getting 2 values they are return value(true/false) ,im i.e frame\n",
    "        im,results=mp_detect(frame,holistic)\n",
    "        print(results)\n",
    "        landmarks_style(im,results)        #draw landmarks\n",
    "        cv2.imshow('opencv feed',im)     #showing what has read  the user.\"opencv feed\" is name given to frame\n",
    "        if cv2.waitKey(10) & 0xFF==ord('p'):#breaking the loop.  if the current key pressed is 'q' the loop breaks\n",
    "            break                                                           \n",
    "    cap.release()                           #releases the webcam\n",
    "    cv2.destroyAllWindows()                 #closes all opencv windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de67ded1-6a0f-4575-adb3-ba2306f9f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEYPOINT EXTRsign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d58ecced-c94a-4f12-a1e8-3e42630c42f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.array is used to create arrays\n",
    "#pose_landmarks has x,y,z,visibility rest has only x,y,z\n",
    "#results.pose_landmarks give the result in form of landmark{x:,y:,z:,visibility:} \n",
    "#result.pose_lanmarks.landmark give the result in form of list i.e [x: y: z: visibility:, etc]\n",
    "#using flatten converts into 1-D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd3651b7-b92f-41dc-b16f-7be01acaf523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keypoints_extract(results):   # function to extract key points \n",
    "    pose=np.array([[res.x,res.y,res.z,res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "    lh=np.array([[res.x,res.y,res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh=np.array([[res.x,res.y,res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose,face,lh,rh]) #cancatenating arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ee6bba-fd34-43ca-be32-634736734d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_extract(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1c5515-6ef7-425c-ac4e-2a8ac0299a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_extract(results).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "948015d0-4d48-4444-a943-5bb517428fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING SUBDIRS FOR CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c2b3e40-419a-4c2c-95c7-afb9dca104d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=os.path.join('ISL_DATASET') #PATH is path where exported data is stored i.e numpy arrays,here the exported data is extracted keypoints\n",
    "signs=np.array(['hello','thanks','iloveyou','sorry','no','goodbye']) #detecting the signshello ,thanks,iloveyou\n",
    "no_seq=30                                 #using 30 frames do detect the sign,collecting 30 videos for each sign\n",
    "seq_len=30                              #each video is of length 30 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99298cf7-01d9-4586-b52f-c46289205edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ISL_DATASET is directory\n",
    "#sign detection means takes seq of data to predict the sign rather than single frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b2b94f9-7656-4a24-87ee-35bd2b314a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating folders to store the data\n",
    "for sign in signs:  #it iterates over every sign\n",
    "    for seq in range(no_seq): #it iterates over range of numbers from o to 29(no_seq-1)\n",
    "        try:\n",
    "            os.makedirs(os.path.join(PATH,sign,str(seq)))  #it is creating directory for each combination of sign and seq number \n",
    "        except:                                                        #using os.makedirs()\n",
    "            pass                                                        #if directory exists it goes to except block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9129d1d4-212b-4185-992b-4ba1a8d90955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLLECTING PERSONALISED DATA SAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d793ee7-02aa-4e2b-b9e4-e5e9e76c50f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap =cv2.VideoCapture(0) \n",
    "with holistics.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
    "    for sign in signs:\n",
    "        for seq in range(no_seq):\n",
    "            for frame_num in range(seq_len):\n",
    "                    ret,frame=cap.read()     #each frame is read from video feed          \n",
    "                    im,results=mp_detect(frame,holistic) #pose detection is performed on the im using the func and return the landmarks\n",
    "                    print(results)   #on the im and pose detection results\n",
    "                    landmarks_style(im,results)   #landmarks are styled on each im    \n",
    "                    cv2.imshow('opencv feed',im)        #displaying the im\n",
    "                    if cv2.waitKey(10) & 0xFF==ord('p'):\n",
    "                        break    \n",
    "                    if frame_num == 0: \n",
    "                             #message displayed started data collection\n",
    "                             cv2.putText(im, 'STARTING COLLECTION', (120,200), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                             cv2.putText(im, 'Collecting frames for {} Video Number {}'.format(sign, seq), (15,12), \n",
    "                                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                             # Show to screen\n",
    "                             cv2.imshow('OpenCV Feed', im)\n",
    "                             cv2.waitKey(2000)\n",
    "                    else:\n",
    "                  \n",
    "                             #displays the ongoing data collection\n",
    "                             cv2.putText(im, 'Collecting frames for {} Video Number {}'.format(sign, seq), (15,12), \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                             # Show to screen\n",
    "                             cv2.imshow('OpenCV Feed', im)\n",
    "                    \n",
    "                    keypoints = keypoints_extract(results)\n",
    "                    npy_path = os.path.join(PATH, sign, str(seq), str(frame_num))\n",
    "                    np.save(npy_path, keypoints) #keypoints extraced from pose detection are saved in numpy arrays and stored in corresponding directory \n",
    "                                                #of sign,seq number,frame\n",
    "    cap.release()        #releases video object                   \n",
    "    cv2.destroyAllWindows()      #open cv windows are destroyed           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ba004-78bc-4512-be2a-d146f1cda92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collection break: having breaks between each seq collection allows you to reset and reposition yourself to collect the sign from start to finish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c96ea1-3792-4624-9b16-4c260fd24f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data create labels and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b49c9749-f3a4-447a-877a-06997cb2925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical # to_categorical : conerts integer into categorical data\n",
    "from sklearn.model_selection import train_test_split # train_test_split : splits the dataset into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c92a334-af90-46a1-bddf-19e5f7ea1252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a label array that represents signs\n",
    "labels_map={label:num for num,label in enumerate(signs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e6c789-23b4-48a3-b8c1-68390dba9141",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d13d2e72-e061-425c-b035-f52d579196cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs,labels=[] ,[] #seqs represents x-data ,labels represents y-data\n",
    "for sign in signs:\n",
    "    for seq in range(no_seq):\n",
    "        window=[]\n",
    "        for frame_num in range(seq_len):\n",
    "            res=np.load(os.path.join(PATH,sign,str(seq),\"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        seqs.append(window)\n",
    "        labels.append(labels_map[sign])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b823dc0-26db-4f93-8b93-487f79614665",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6240641-f2d4-4f1b-836f-263bdb0704e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocesing\n",
    "X=np.array(seqs)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89150b57-58b6-4617-8c2a-8be8be14b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f54309f-92b2-47fe-9b5b-84e4a0eae43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=to_categorical(labels).astype(int) #binary format\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "14d51c3b-f0f6-423d-8744-749bb3f6d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eb3575-ee49-46cd-b9b2-fb79b27f3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,X_test.shape ,y_train.shape,y_test.shape#training and testing tuples size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "94787f45-dc51-41c4-8295-3e4346f329fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0db11b85-e584-4303-9746-8e74ea5058f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense\n",
    "#LSTM is a rnn layer which helps in handling the sequential data\n",
    "#Dense is a fully connected neural network layer.each neuron is connected to each neuron in previous layer ,helps in learning the complex patterns\n",
    "#TensoBoard is a visualization tool for monitoring and visualizing various aspects like model training and evaluation\n",
    "#sequential helps to build neural networks layer by layer in a sequential manner.layers are added to model one by one \n",
    "#and each is connected to previous layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6e98382a-4ba9-47f9-a84a-2e2f55eaca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "logS_dir = os.path.join('Logs')#creates the directory path named logs\n",
    "tb_callback = TensorBoard(log_dir=logS_dir) # tb_callback is a tensorboard callback object.tensorboard writes the log files at path logs_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f69a67fd-3c80-495c-8a83-c8286857740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model\n",
    "#A sequential model architecture consists of multiple LSTM layers followed by dense layers.the activation function 'relu' is\n",
    "#used in LSTM and dense layers except last dense layer that uses softmax activation function.softmax is used for multi class classification\n",
    "#the model is designed to take input seq 30 with 1662 features\n",
    "#return_seq =false means final output should be returned ,true means it should return seq of outputs not only final output \n",
    "#64,128,32 means neurons in the layer\n",
    "#activation functions decide whether the neuron should be activated or not.they are applied on output of each neuron\n",
    "#Relu =max(0,x)\n",
    "#softmax used in output layer in multi classification models to produce probabilites of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "192d1e89-8c5b-45ec-b269-1fdff7a1801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential() #sequential model\n",
    "model.add(LSTM(64,return_sequences=True,activation='relu',input_shape=(30,1662))) \n",
    "model.add(LSTM(128,return_sequences=True,activation='relu'))\n",
    "model.add(LSTM(64,return_sequences=False,activation='relu'))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(signs.shape[0],activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "acb16554-9a7a-40eb-bcf2-60b13c18ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['categorical_accuracy']) #model compiling\n",
    "#optimizer:updates the weigths of neural network during training,Adam is an algorithm\n",
    "#loss function:it finds the difference between actual and predicted output.categorical_crossentropy is a loss function used for multiclassclassification\n",
    "#metrics:used to evaluate the performance of model.metric used is categorical_accuracy.it is used in multi class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaf2f1b-3a48-4d4e-9f11-e3a6d9245cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(X_train,y_train,epochs=200,validation_split=0.2,callbacks=[tb_callback])\n",
    "#epochs:The number of times the entire training dataset will be passed forward and backward through the neural network during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3b1e34d0-e9fd-48a5-b507-1bc29176601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [.7, 0.1, 0.2,.3,.5,.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cfbf6e-a014-4309-8788-8f761814253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "signs[np.argmax(res)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6f02a2-baae-4223-b57c-883bd4964bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "signs.shape[0] #output layer has 3 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ad19b4-1bc5-494d-bc2b-c63aa366a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4f5a0de5-3483-4e7e-b8c5-25f4ef5d8b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527eef4b-104d-4966-af6d-2d66b5d80035",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=model.predict(X_test)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efde17e-306e-4f39-ab9b-7b114dba15a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "signs[np.argmax(res[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "94e609ef-c355-47de-8706-e322ba128f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c97779f7-9cd0-43bb-a321-c941d06db35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sign.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d57c442e-6da4-4c1c-9571-920886821cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "34ae70eb-3a27-4fb9-80b6-bd2554eca83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d8b64-2659-41c6-8d5c-667bdf389afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e3b0a282-ee28-4b61-8daa-b6367bc17744",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbf75d7-f195-4071-b8ec-49a6519593f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a69d79-5f62-4c3f-b8bd-9d517252bb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytrue, yhat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3eb0560c-e495-4c9f-99a4-7fbac1eb1b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 11 test in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1bb90d2a-db68-4028-ba99-22dd0fe34a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "color = [(245,117,16), (117,245,16), (16,117,245),(16,119,245),(16,117,243),(17,117,245)]\n",
    "def prob_viz(res, signs, input_frame, color):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), color[num], -1)\n",
    "        cv2.putText(output_frame, signs[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1396cda8-4225-4079-9bfe-7194dc6ebef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. New detection variables\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "seq = []\n",
    "sentence = []\n",
    "threshold = 0.4\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with holistics.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        im, results = mp_detect(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "     \n",
    "        landmarks_style(im, results)\n",
    "        \n",
    "        \n",
    "        keypoints = keypoints_extract(results)\n",
    "           #seq.insert(0,keypoints)\n",
    "          #seq = seq[:30]\n",
    "        seq.append(keypoints)\n",
    "        seq = seq[-30:]\n",
    "        \n",
    "        if len(seq) == 30:\n",
    "            res = model.predict(np.expand_dims(seq, axis=0))[0]\n",
    "            print(signs[np.argmax(res)])\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if res[np.argmax(res)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if signs[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(signs[np.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(signs[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "         \n",
    "            im = prob_viz(res, signs, im, color)\n",
    "            \n",
    "        cv2.rectangle(im, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(im, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', im)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('p'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5efa06e-f24e-47a8-bf76-b12bd51dfa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "precision = precision_score(y_test_labels, y_pred_labels, average='weighted')\n",
    "recall = recall_score(y_test_labels, y_pred_labels, average='weighted')\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8e369-b4e8-48df-ab56-6a058352c7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_test_labels, y_pred_labels, labels=range(len(signs)), target_names=signs)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db9b627-3cf9-4920-ab3c-30849e37949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd425ed-6267-450e-9f2e-800336df616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9985cb-cea6-42db-9b46-4b273207f571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
